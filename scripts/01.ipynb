{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02dd7b09-39dd-4f2d-84e2-9005c0482904",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -r ../lake/*\n",
    "! rm -r ../spark-warehouse/*\n",
    "! rm -r ../metastore_db/*\n",
    "! rm -r ../temp/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bf8aa7a-5d14-462e-9b05-ef27feef4f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['SPARK_VERSION'] = '3.5.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e22f5605-5431-450c-b004-00ca6001b0d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T09:16:28.238022Z",
     "start_time": "2024-10-14T09:16:28.221689Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta import *\n",
    "import pydeequ\n",
    "from pydeequ.analyzers import *\n",
    "\n",
    "spark = SparkSession.builder.master('local[*]').appName('quick-start') \\\n",
    "    .config('spark.sql.extensions', 'io.delta.sql.DeltaSparkSessionExtension') \\\n",
    "    .config('spark.sql.catalog.spark_catalog', 'org.apache.spark.sql.delta.catalog.DeltaCatalog') \\\n",
    "    .config('spark.jars.packages', pydeequ.deequ_maven_coord) \\\n",
    "    .config('spark.jars.excludes', pydeequ.f2j_maven_coord) \\\n",
    "    .config('spark.sql.warehouse.dir', '../spark-warehouse') \\\n",
    "    .config('spark.driver.extraJavaOptions', '-Dderby.system.home=\"../metastore_db/\"') \\\n",
    "    .config('spark.driver.memory', '10g') \\\n",
    "    .config('spark.driver.maxResultSize', '10g') \\\n",
    "    .config('spark.sql.repl.eagerEval.enabled', True) \\\n",
    "    .config('spark.databricks.delta.schema.autoMerge.enabled', True) \\\n",
    "    .config('spark.databricks.delta.autoCompact.enabled', True) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2d44acf-d819-4963-8dc8-896d4fe9778e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|database_name|\n",
      "+-------------+\n",
      "|default      |\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT namespace as database_name FROM {df}',df = spark.sql('SHOW DATABASES')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "394ff16a-5da8-436d-b627-434b1e0ff1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|database_name |\n",
      "+--------------+\n",
      "|adventureworks|\n",
      "|default       |\n",
      "|otherdb       |\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('CREATE DATABASE IF NOT EXISTS AdventureWorks')\n",
    "spark.sql('CREATE DATABASE IF NOT EXISTS OtherDB')\n",
    "\n",
    "spark.sql('SELECT namespace as database_name FROM {df}',df = spark.sql('SHOW DATABASES')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b31566e7-07c4-4ed8-b37b-cc5d60ee5752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|current_database|\n",
      "+----------------+\n",
      "|adventureworks  |\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('USE AdventureWorks')\n",
    "spark.sql('SELECT CURRENT_SCHEMA() AS current_database').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20771810-413d-479c-911c-3f3c015cd46a",
   "metadata": {},
   "source": [
    "## Load data into Bronze zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab0a468f-5203-4790-8e96-49b705539074",
   "metadata": {},
   "outputs": [],
   "source": [
    "jdbc_options = {\n",
    "      'url': 'jdbc:sqlserver://localhost:1433;database=AdventureWorks2022;trustServerCertificate=true',\n",
    "      'driver': 'com.microsoft.sqlserver.jdbc.SQLServerDriver',\n",
    "      'user': 'SA',\n",
    "      'password': 'PiIs&&&31415926535'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd2dce30-a5c3-4a36-ad80-875f2ab3a531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+---------+\n",
      "|column_name          |data_type|\n",
      "+---------------------+---------+\n",
      "|BusinessEntityID     |int      |\n",
      "|PersonType           |string   |\n",
      "|NameStyle            |boolean  |\n",
      "|Title                |string   |\n",
      "|FirstName            |string   |\n",
      "|MiddleName           |string   |\n",
      "|LastName             |string   |\n",
      "|Suffix               |string   |\n",
      "|EmailPromotion       |int      |\n",
      "|AdditionalContactInfo|string   |\n",
      "|Demographics         |string   |\n",
      "|rowguid              |string   |\n",
      "|ModifiedDate         |timestamp|\n",
      "+---------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qryStr = '(SELECT * FROM Person.Person) t'\n",
    "person_df = spark.read.format('jdbc').option('dbtable', qryStr ).options(**jdbc_options).load()\n",
    "spark.createDataFrame(person_df.dtypes, ['column_name', 'data_type']).show(person_df.count(), truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8bfb56f-2e8d-4bd4-922e-c25c061ee539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+\n",
      "|database_name |table_name|\n",
      "+--------------+----------+\n",
      "|adventureworks|person    |\n",
      "+--------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = '''\n",
    "CREATE TABLE IF NOT EXISTS AdventureWorks.Person (\n",
    "    BusinessEntityID INT,\n",
    "    PersonType STRING,\n",
    "    Title STRING,\n",
    "    FirstName STRING,\n",
    "    MiddleName STRING,\n",
    "    LastName STRING,\n",
    "    Suffix STRING\n",
    ") USING DELTA LOCATION '../lake/bronze/AdventureWorks/Person/Person'\n",
    "'''\n",
    "spark.sql(sql)\n",
    "\n",
    "spark.sql('SELECT namespace as database_name, tableName as table_name FROM {df}', df = spark.sql('SHOW TABLES IN AdventureWorks')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0aaa4b22-f425-4b7f-b314-bc3d0dd9c792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|person_row_count|\n",
      "+----------------+\n",
      "|19972           |\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "    INSERT INTO AdventureWorks.Person\n",
    "    SELECT BusinessEntityID, PersonType, Title, FirstName, MiddleName, LastName, Suffix FROM {df}\n",
    "    ''', df = person_df)\n",
    "\n",
    "spark.sql('SELECT COUNT(*) AS person_row_count FROM AdventureWorks.Person').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5911c774-6cea-49e9-9a54-d2aafb70cad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+\n",
      "|column_name     |data_type|\n",
      "+----------------+---------+\n",
      "|BusinessEntityID|int      |\n",
      "|rowguid         |string   |\n",
      "|ModifiedDate    |timestamp|\n",
      "+----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qryStr = '(SELECT * FROM Person.BusinessEntity) t'\n",
    "entity_df = spark.read.format('jdbc').option('dbtable', qryStr ).options(**jdbc_options).load()\n",
    "spark.createDataFrame(entity_df.dtypes, ['column_name', 'data_type']).show(entity_df.count(), truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a8f2d78-fb39-425a-909e-aa1fb9322add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+\n",
      "|database_name |table_name    |\n",
      "+--------------+--------------+\n",
      "|adventureworks|businessentity|\n",
      "|adventureworks|person        |\n",
      "+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = '''\n",
    "CREATE TABLE IF NOT EXISTS AdventureWorks.BusinessEntity (\n",
    "    BusinessEntityID INT,\n",
    "    ModifiedDate TIMESTAMP\n",
    ") USING DELTA LOCATION '../lake/bronze/AdventureWorks/Person/BusinessEntity'\n",
    "'''\n",
    "\n",
    "spark.sql(sql)\n",
    "spark.sql('SELECT namespace as database_name, tableName as table_name FROM {df}', df = spark.sql('SHOW TABLES IN AdventureWorks')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ae27e97-7a0c-4145-9e22-367565276e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|entity_row_count|\n",
      "+----------------+\n",
      "|20777           |\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "    INSERT INTO AdventureWorks.BusinessEntity\n",
    "    SELECT BusinessEntityID, ModifiedDate\n",
    "    FROM {df}\n",
    "''', df = entity_df)\n",
    "\n",
    "spark.sql('SELECT COUNT(*) AS entity_row_count FROM AdventureWorks.BusinessEntity').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40e2f6ff-8b97-4743-aa80-2ac4297662d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+\n",
      "|column_name     |data_type|\n",
      "+----------------+---------+\n",
      "|BusinessEntityID|int      |\n",
      "|EmailAddressID  |int      |\n",
      "|EmailAddress    |string   |\n",
      "|rowguid         |string   |\n",
      "|ModifiedDate    |timestamp|\n",
      "+----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qryStr = '(SELECT * FROM Person.EmailAddress) t'\n",
    "email_df = spark.read.format('jdbc').option('dbtable', qryStr ).options(**jdbc_options).load()\n",
    "spark.createDataFrame(email_df.dtypes, ['column_name', 'data_type']).show(email_df.count(), truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "236f363e-721f-4c06-94a2-e1317120f69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+\n",
      "|database_name |table_name    |\n",
      "+--------------+--------------+\n",
      "|adventureworks|businessentity|\n",
      "|adventureworks|emailaddress  |\n",
      "|adventureworks|person        |\n",
      "+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = '''\n",
    "CREATE TABLE IF NOT EXISTS AdventureWorks.EmailAddress (\n",
    "    BusinessEntityID INT,\n",
    "    EmailAddress STRING,\n",
    "    ModifiedDate TIMESTAMP\n",
    ") USING DELTA LOCATION '../lake/bronze/AdventureWorks/Person/EmailAddress'\n",
    "'''\n",
    "\n",
    "spark.sql(sql)\n",
    "spark.sql('SELECT namespace as database_name, tableName as table_name FROM {df}', df = spark.sql('SHOW TABLES IN AdventureWorks')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fcacaf4d-381c-4dd4-af0f-61ece52aaf8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|email_row_count|\n",
      "+---------------+\n",
      "|19972          |\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "    INSERT INTO AdventureWorks.EmailAddress\n",
    "    SELECT BusinessEntityID, EmailAddress, ModifiedDate FROM {df}\n",
    "''', df = email_df)\n",
    "\n",
    "spark.sql('SELECT COUNT(*) AS email_row_count FROM AdventureWorks.EmailAddress').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "080b0ce4-83fc-45ac-b5c5-01f60c70fffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------+\n",
      "|column_name      |data_type|\n",
      "+-----------------+---------+\n",
      "|BusinessEntityID |int      |\n",
      "|PhoneNumber      |string   |\n",
      "|PhoneNumberTypeID|int      |\n",
      "|ModifiedDate     |timestamp|\n",
      "+-----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qryStr = '(SELECT * FROM Person.PersonPhone) t'\n",
    "phone_df = spark.read.format('jdbc').option('dbtable', qryStr ).options(**jdbc_options).load()\n",
    "spark.createDataFrame(phone_df.dtypes, ['column_name', 'data_type']).show(phone_df.count(), truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5e60f0b-3bdc-4d81-8d82-8a09f75bb5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+\n",
      "|database_name |table_name    |\n",
      "+--------------+--------------+\n",
      "|adventureworks|businessentity|\n",
      "|adventureworks|emailaddress  |\n",
      "|adventureworks|person        |\n",
      "|adventureworks|personphone   |\n",
      "+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = '''\n",
    "CREATE TABLE IF NOT EXISTS AdventureWorks.PersonPhone (\n",
    "    BusinessEntityID INT,\n",
    "    PhoneNumber STRING,\n",
    "    ModifiedDate TIMESTAMP\n",
    ") USING DELTA LOCATION '../lake/bronze/AdventureWorks/Person/PersonPhone'\n",
    "'''\n",
    "\n",
    "spark.sql(sql)\n",
    "spark.sql('SELECT namespace as database_name, tableName as table_name FROM {df}', df = spark.sql('SHOW TABLES IN AdventureWorks')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f130f993-30b7-4984-9818-be72c20ff055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|phone_row_count|\n",
      "+---------------+\n",
      "|19972          |\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "    INSERT INTO AdventureWorks.PersonPhone\n",
    "    SELECT BusinessEntityID, PhoneNumber, ModifiedDate FROM {df}\n",
    "''', df = phone_df)\n",
    "\n",
    "spark.sql('SELECT COUNT(*) AS phone_row_count FROM AdventureWorks.PersonPhone').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2311de5-826f-49d3-a213-eac72f1c04eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------+-----+---------+----------+----------+------+----------------------------+------------+\n",
      "|BusinessEntityID|PersonType|Title|FirstName|MiddleName|LastName  |Suffix|EmailAddress                |PhoneNumber |\n",
      "+----------------+----------+-----+---------+----------+----------+------+----------------------------+------------+\n",
      "|1               |EM        |NULL |Ken      |J         |Sánchez   |NULL  |ken0@adventure-works.com    |697-555-0142|\n",
      "|2               |EM        |NULL |Terri    |Lee       |Duffy     |NULL  |terri0@adventure-works.com  |819-555-0175|\n",
      "|3               |EM        |NULL |Roberto  |NULL      |Tamburello|NULL  |roberto0@adventure-works.com|212-555-0187|\n",
      "|4               |EM        |NULL |Rob      |NULL      |Walters   |NULL  |rob0@adventure-works.com    |612-555-0100|\n",
      "|5               |EM        |Ms.  |Gail     |A         |Erickson  |NULL  |gail0@adventure-works.com   |849-555-0139|\n",
      "+----------------+----------+-----+---------+----------+----------+------+----------------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flat_query = '''\n",
    "SELECT\n",
    "    be.BusinessEntityID,\n",
    "    p.PersonType,\n",
    "    p.Title,\n",
    "    p.FirstName,\n",
    "    p.MiddleName,\n",
    "    p.LastName,\n",
    "    p.Suffix,\n",
    "    ea.EmailAddress,\n",
    "    pp.PhoneNumber\n",
    "FROM AdventureWorks.BusinessEntity be \n",
    "INNER JOIN AdventureWorks.Person p ON be.BusinessEntityID = p.BusinessEntityID\n",
    "INNER JOIN AdventureWorks.EmailAddress ea ON ea.BusinessEntityID = be.BusinessEntityID \n",
    "INNER JOIN AdventureWorks.PersonPhone pp ON pp.BusinessEntityID = be.BusinessEntityID\n",
    "'''\n",
    "\n",
    "spark.sql(flat_query).limit(5).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd4f0928-c45f-4bba-874a-9765670503c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-------------+\n",
      "|column_name          |data_type    |\n",
      "+---------------------+-------------+\n",
      "|SalesOrderID         |int          |\n",
      "|SalesOrderDetailID   |int          |\n",
      "|CarrierTrackingNumber|string       |\n",
      "|OrderQty             |smallint     |\n",
      "|ProductID            |int          |\n",
      "|SpecialOfferID       |int          |\n",
      "|UnitPrice            |decimal(19,4)|\n",
      "|UnitPriceDiscount    |decimal(19,4)|\n",
      "|LineTotal            |decimal(38,6)|\n",
      "|rowguid              |string       |\n",
      "|ModifiedDate         |timestamp    |\n",
      "+---------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qryStr = '(SELECT * FROM Sales.SalesOrderDetail) t'\n",
    "sod_df = spark.read.format('jdbc').option('dbtable', qryStr ).options(**jdbc_options).load()\n",
    "spark.createDataFrame(sod_df.dtypes, ['column_name', 'data_type']).show(sod_df.count(), truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "130a3a92-6e13-42fe-9503-91587752cc20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+\n",
      "|database_name |table_name      |\n",
      "+--------------+----------------+\n",
      "|adventureworks|businessentity  |\n",
      "|adventureworks|emailaddress    |\n",
      "|adventureworks|person          |\n",
      "|adventureworks|personphone     |\n",
      "|adventureworks|salesorderdetail|\n",
      "+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = '''\n",
    "CREATE TABLE IF NOT EXISTS AdventureWorks.SalesOrderDetail (\n",
    "    SalesOrderDetailID INT,\n",
    "    SalesOrderID INT,\n",
    "    OrderQty SMALLINT,\n",
    "    ProductID INT,\n",
    "    UnitPrice DECIMAL(19,4),\n",
    "    UnitPriceDiscount DECIMAL(19,4),\n",
    "    ModifiedDate TIMESTAMP\n",
    ") USING DELTA LOCATION '../lake/bronze/AdventureWorks/Sales/SalesOrderDetail'\n",
    "'''\n",
    "\n",
    "spark.sql(sql)\n",
    "spark.sql('SELECT namespace as database_name, tableName as table_name FROM {df}', df = spark.sql('SHOW TABLES IN AdventureWorks')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe849e60-b736-4d1b-85d6-9fd9ff86c484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|sod_row_count|\n",
      "+-------------+\n",
      "|121317       |\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "    INSERT INTO AdventureWorks.SalesOrderDetail\n",
    "    SELECT SalesOrderDetailID, SalesOrderID, OrderQty, ProductID, UnitPrice, UnitPriceDiscount, ModifiedDate\n",
    "    FROM {df}\n",
    "''', df = sod_df)\n",
    "\n",
    "spark.sql('SELECT COUNT(*) AS sod_row_count FROM AdventureWorks.SalesOrderDetail').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dfce5887-0399-4b3b-a485-9c024a72988b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-------------+\n",
      "|column_name           |data_type    |\n",
      "+----------------------+-------------+\n",
      "|SalesOrderID          |int          |\n",
      "|RevisionNumber        |int          |\n",
      "|OrderDate             |timestamp    |\n",
      "|DueDate               |timestamp    |\n",
      "|ShipDate              |timestamp    |\n",
      "|Status                |int          |\n",
      "|OnlineOrderFlag       |boolean      |\n",
      "|SalesOrderNumber      |string       |\n",
      "|PurchaseOrderNumber   |string       |\n",
      "|AccountNumber         |string       |\n",
      "|CustomerID            |int          |\n",
      "|SalesPersonID         |int          |\n",
      "|TerritoryID           |int          |\n",
      "|BillToAddressID       |int          |\n",
      "|ShipToAddressID       |int          |\n",
      "|ShipMethodID          |int          |\n",
      "|CreditCardID          |int          |\n",
      "|CreditCardApprovalCode|string       |\n",
      "|CurrencyRateID        |int          |\n",
      "|SubTotal              |decimal(19,4)|\n",
      "|TaxAmt                |decimal(19,4)|\n",
      "|Freight               |decimal(19,4)|\n",
      "|TotalDue              |decimal(19,4)|\n",
      "|Comment               |string       |\n",
      "|rowguid               |string       |\n",
      "|ModifiedDate          |timestamp    |\n",
      "+----------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qryStr = '(SELECT * FROM Sales.SalesOrderHeader) t'\n",
    "soh_df = spark.read.format('jdbc').option('dbtable', qryStr ).options(**jdbc_options).load()\n",
    "spark.createDataFrame(soh_df.dtypes, ['column_name', 'data_type']).show(soh_df.count(), truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64a32246-9656-48c1-b4b3-de736bf1b9c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+\n",
      "|database_name |table_name      |\n",
      "+--------------+----------------+\n",
      "|adventureworks|businessentity  |\n",
      "|adventureworks|emailaddress    |\n",
      "|adventureworks|person          |\n",
      "|adventureworks|personphone     |\n",
      "|adventureworks|salesorderdetail|\n",
      "|adventureworks|salesorderheader|\n",
      "+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = '''\n",
    "CREATE TABLE IF NOT EXISTS AdventureWorks.SalesOrderHeader (\n",
    "    SalesOrderID INT,\n",
    "    OrderDate TIMESTAMP,\n",
    "    ShipDate TIMESTAMP,\n",
    "    DueDate TIMESTAMP,\n",
    "    OnlineOrderFlag BOOLEAN,\n",
    "    CustomerID INT,\n",
    "    TerritoryID INT,\n",
    "    TaxAmt DECIMAL(19,4),\n",
    "    Freight DECIMAL(19,4),\n",
    "    TotalDue DECIMAL(19,4),\n",
    "    ModifiedDate TIMESTAMP\n",
    ") USING DELTA LOCATION '../lake/bronze/AdventureWorks/Sales/SalesOrderHeader'\n",
    "'''\n",
    "\n",
    "spark.sql(sql)\n",
    "spark.sql('SELECT namespace as database_name, tableName as table_name FROM {df}', df = spark.sql('SHOW TABLES IN AdventureWorks')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b8f317f5-8bee-4f8f-8959-8689a1cb2d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|soh_row_count|\n",
      "+-------------+\n",
      "|31465        |\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "    INSERT INTO AdventureWorks.SalesOrderHeader\n",
    "    SELECT\n",
    "        SalesOrderID, OrderDate, ShipDate, DueDate, OnlineOrderFlag, CustomerID, TerritoryID, TaxAmt, Freight, TotalDue, ModifiedDate\n",
    "    FROM {df}\n",
    "''', df = soh_df)\n",
    "\n",
    "spark.sql('SELECT COUNT(*) AS soh_row_count FROM AdventureWorks.SalesOrderHeader').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "493abc33-8de6-4fdb-98c9-0c3db12770eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+\n",
      "|column_name  |data_type|\n",
      "+-------------+---------+\n",
      "|CustomerID   |int      |\n",
      "|PersonID     |int      |\n",
      "|StoreID      |int      |\n",
      "|TerritoryID  |int      |\n",
      "|AccountNumber|string   |\n",
      "|rowguid      |string   |\n",
      "|ModifiedDate |timestamp|\n",
      "+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qryStr = '(SELECT * FROM Sales.Customer) t'\n",
    "customer_df = spark.read.format('jdbc').option('dbtable', qryStr ).options(**jdbc_options).load()\n",
    "spark.createDataFrame(customer_df.dtypes, ['column_name', 'data_type']).show(customer_df.count(), truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9984e010-e6d0-4624-b892-f322ac99b924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+\n",
      "|database_name |table_name      |\n",
      "+--------------+----------------+\n",
      "|adventureworks|businessentity  |\n",
      "|adventureworks|customer        |\n",
      "|adventureworks|emailaddress    |\n",
      "|adventureworks|person          |\n",
      "|adventureworks|personphone     |\n",
      "|adventureworks|salesorderdetail|\n",
      "|adventureworks|salesorderheader|\n",
      "+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = '''\n",
    "CREATE TABLE IF NOT EXISTS AdventureWorks.Customer (\n",
    "    CustomerID INT,\n",
    "    PersonID INT,\n",
    "    StoreID INT,\n",
    "    TerritoryID INT,\n",
    "    ModifiedDate TIMESTAMP\n",
    ") USING DELTA LOCATION '../lake/bronze/AdventureWorks/Sales/Customer'\n",
    "'''\n",
    "\n",
    "spark.sql(sql)\n",
    "spark.sql('SELECT namespace as database_name, tableName as table_name FROM {df}', df = spark.sql('SHOW TABLES IN AdventureWorks')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7702b93a-55e6-490f-917b-084bb29d42d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|customer_row_count|\n",
      "+------------------+\n",
      "|19820             |\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "    INSERT INTO AdventureWorks.Customer\n",
    "    SELECT CustomerID, PersonID, StoreID, TerritoryID, ModifiedDate FROM {df}\n",
    "''', df = customer_df)\n",
    "\n",
    "spark.sql('SELECT COUNT(*) AS customer_row_count FROM AdventureWorks.Customer').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2593c1ad-22c6-450c-bd93-a1f272a582b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+-----------+---------+--------+---------+\n",
      "|OrderDate          |CustomerID|TerritoryID|ProductID|OrderQty|UnitPrice|\n",
      "+-------------------+----------+-----------+---------+--------+---------+\n",
      "|2011-05-31 00:00:00|29825     |5          |776      |1       |2024.9940|\n",
      "|2011-05-31 00:00:00|29825     |5          |777      |3       |2024.9940|\n",
      "|2011-05-31 00:00:00|29825     |5          |778      |1       |2024.9940|\n",
      "|2011-05-31 00:00:00|29825     |5          |771      |1       |2039.9940|\n",
      "+-------------------+----------+-----------+---------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flat_query = '''\n",
    "SELECT\n",
    "    soh.OrderDate, soh.CustomerID, soh.TerritoryID, sod.ProductID, sod.OrderQty, sod.UnitPrice \n",
    "FROM AdventureWorks.SalesOrderHeader soh \n",
    "INNER JOIN AdventureWorks.SalesOrderDetail sod ON soh.SalesOrderID = sod.SalesOrderID \n",
    "INNER JOIN AdventureWorks.Customer c ON c.CustomerID = soh.CustomerID \n",
    "'''\n",
    "\n",
    "spark.sql(flat_query).limit(4).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "14ea9410-768a-4496-99b2-5eb53104c029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-------------+\n",
      "|column_name          |data_type    |\n",
      "+---------------------+-------------+\n",
      "|ProductID            |int          |\n",
      "|Name                 |string       |\n",
      "|ProductNumber        |string       |\n",
      "|MakeFlag             |boolean      |\n",
      "|FinishedGoodsFlag    |boolean      |\n",
      "|Color                |string       |\n",
      "|SafetyStockLevel     |smallint     |\n",
      "|ReorderPoint         |smallint     |\n",
      "|StandardCost         |decimal(19,4)|\n",
      "|ListPrice            |decimal(19,4)|\n",
      "|Size                 |string       |\n",
      "|SizeUnitMeasureCode  |string       |\n",
      "|WeightUnitMeasureCode|string       |\n",
      "|Weight               |decimal(8,2) |\n",
      "|DaysToManufacture    |int          |\n",
      "|ProductLine          |string       |\n",
      "|Class                |string       |\n",
      "|Style                |string       |\n",
      "|ProductSubcategoryID |int          |\n",
      "|ProductModelID       |int          |\n",
      "|SellStartDate        |timestamp    |\n",
      "|SellEndDate          |timestamp    |\n",
      "|DiscontinuedDate     |timestamp    |\n",
      "|rowguid              |string       |\n",
      "|ModifiedDate         |timestamp    |\n",
      "+---------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qryStr = '(SELECT * FROM Production.Product) t'\n",
    "product_df = spark.read.format('jdbc').option('dbtable', qryStr ).options(**jdbc_options).load()\n",
    "spark.createDataFrame(product_df.dtypes, ['column_name', 'data_type']).show(product_df.count(), truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "81eae63d-990f-45fb-915e-916b88b45394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+\n",
      "|database_name |table_name      |\n",
      "+--------------+----------------+\n",
      "|adventureworks|businessentity  |\n",
      "|adventureworks|customer        |\n",
      "|adventureworks|emailaddress    |\n",
      "|adventureworks|person          |\n",
      "|adventureworks|personphone     |\n",
      "|adventureworks|product         |\n",
      "|adventureworks|salesorderdetail|\n",
      "|adventureworks|salesorderheader|\n",
      "+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = '''\n",
    "CREATE TABLE IF NOT EXISTS AdventureWorks.Product (\n",
    "    ProductID INT,\n",
    "    ProductSubcategoryID INT,\n",
    "    Name STRING,\n",
    "    Color STRING,\n",
    "    ListPrice DECIMAL(19,4),\n",
    "    Size STRING,\n",
    "    Weight DECIMAL(8,2),\n",
    "    ModifiedDate TIMESTAMP\n",
    ") USING DELTA LOCATION '../lake/bronze/AdventureWorks/Production/Product'\n",
    "'''\n",
    "\n",
    "spark.sql(sql)\n",
    "spark.sql('SELECT namespace as database_name, tableName as table_name FROM {df}', df = spark.sql('SHOW TABLES IN AdventureWorks')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6e2af065-0f5b-4231-83e5-7d2279956195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|product_row_count|\n",
      "+-----------------+\n",
      "|504              |\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "    INSERT INTO AdventureWorks.Product\n",
    "    SELECT ProductID, ProductSubcategoryID, Name, Color, ListPrice, Size, Weight, ModifiedDate FROM {df}\n",
    "''', df = product_df)\n",
    "\n",
    "spark.sql('SELECT COUNT(*) AS product_row_count FROM AdventureWorks.Product').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d1337ae0-4bb3-4f93-b51b-e40355d377cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------+\n",
      "|column_name      |data_type|\n",
      "+-----------------+---------+\n",
      "|ProductCategoryID|int      |\n",
      "|Name             |string   |\n",
      "|rowguid          |string   |\n",
      "|ModifiedDate     |timestamp|\n",
      "+-----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qryStr = '(SELECT * FROM Production.ProductCategory) t'\n",
    "category_df = spark.read.format('jdbc').option('dbtable', qryStr ).options(**jdbc_options).load()\n",
    "spark.createDataFrame(category_df.dtypes, ['column_name', 'data_type']).show(category_df.count(), truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a9a91d86-863a-4163-87fa-cebd73d9e45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+\n",
      "|database_name |table_name      |\n",
      "+--------------+----------------+\n",
      "|adventureworks|businessentity  |\n",
      "|adventureworks|customer        |\n",
      "|adventureworks|emailaddress    |\n",
      "|adventureworks|person          |\n",
      "|adventureworks|personphone     |\n",
      "|adventureworks|product         |\n",
      "|adventureworks|productcategory |\n",
      "|adventureworks|salesorderdetail|\n",
      "|adventureworks|salesorderheader|\n",
      "+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = '''\n",
    "CREATE TABLE IF NOT EXISTS AdventureWorks.ProductCategory (\n",
    "    ProductCategoryID INT,\n",
    "    Name STRING,\n",
    "    ModifiedDate TIMESTAMP\n",
    ") USING DELTA LOCATION '../lake/bronze/AdventureWorks/Production/ProductCategory'\n",
    "'''\n",
    "\n",
    "spark.sql(sql)\n",
    "spark.sql('SELECT namespace as database_name, tableName as table_name FROM {df}', df = spark.sql('SHOW TABLES IN AdventureWorks')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "02222696-b8af-45c5-91a2-7d75cb537b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|category_row_count|\n",
      "+------------------+\n",
      "|4                 |\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "    INSERT INTO AdventureWorks.ProductCategory\n",
    "    SELECT ProductCategoryID, Name, ModifiedDate FROM {df}\n",
    "''', df = category_df)\n",
    "\n",
    "spark.sql('SELECT COUNT(*) AS category_row_count FROM AdventureWorks.ProductCategory').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f24c1c44-c805-4c56-a9b2-9729221587b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|column_name         |data_type|\n",
      "+--------------------+---------+\n",
      "|ProductSubcategoryID|int      |\n",
      "|ProductCategoryID   |int      |\n",
      "|Name                |string   |\n",
      "|rowguid             |string   |\n",
      "|ModifiedDate        |timestamp|\n",
      "+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qryStr = '(SELECT * FROM Production.ProductSubcategory) t'\n",
    "subcategory_df = spark.read.format('jdbc').option('dbtable', qryStr ).options(**jdbc_options).load()\n",
    "spark.createDataFrame(subcategory_df.dtypes, ['column_name', 'data_type']).show(subcategory_df.count(), truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7ac22992-8fc7-468c-8cfc-bc36b9fc3020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+\n",
      "|database_name |table_name        |\n",
      "+--------------+------------------+\n",
      "|adventureworks|businessentity    |\n",
      "|adventureworks|customer          |\n",
      "|adventureworks|emailaddress      |\n",
      "|adventureworks|person            |\n",
      "|adventureworks|personphone       |\n",
      "|adventureworks|product           |\n",
      "|adventureworks|productcategory   |\n",
      "|adventureworks|productsubcategory|\n",
      "|adventureworks|salesorderdetail  |\n",
      "|adventureworks|salesorderheader  |\n",
      "+--------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = '''\n",
    "CREATE TABLE IF NOT EXISTS AdventureWorks.ProductSubcategory (\n",
    "    ProductSubcategoryID INT,\n",
    "    ProductCategoryID INT,\n",
    "    Name STRING,\n",
    "    ModifiedDate TIMESTAMP\n",
    ") USING DELTA LOCATION '../lake/bronze/AdventureWorks/Production/ProductSubcategory'\n",
    "'''\n",
    "\n",
    "spark.sql(sql)\n",
    "spark.sql('SELECT namespace as database_name, tableName as table_name FROM {df}', df = spark.sql('SHOW TABLES IN AdventureWorks')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "029bcc6a-4336-4647-9f08-349bb603edb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|subcategory_row_count|\n",
      "+---------------------+\n",
      "|37                   |\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "    INSERT INTO AdventureWorks.ProductSubcategory\n",
    "    SELECT ProductSubcategoryID, ProductCategoryID, Name, ModifiedDate FROM {df}\n",
    "''', df = subcategory_df)\n",
    "\n",
    "spark.sql('SELECT COUNT(*) AS subcategory_row_count FROM AdventureWorks.ProductSubcategory').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "85ec3ad5-58bd-4789-a9b9-c982e80cad03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------+----------------------+-----+----+------+\n",
      "|CategoryName|SubCategoryName|ProductName           |Color|Size|Weight|\n",
      "+------------+---------------+----------------------+-----+----+------+\n",
      "|Bikes       |Mountain Bikes |Mountain-500 Black, 52|Black|52  |28.68 |\n",
      "|Bikes       |Mountain Bikes |Mountain-500 Black, 48|Black|48  |28.42 |\n",
      "|Bikes       |Mountain Bikes |Mountain-500 Black, 44|Black|44  |28.13 |\n",
      "|Bikes       |Mountain Bikes |Mountain-500 Black, 42|Black|42  |27.77 |\n",
      "|Bikes       |Mountain Bikes |Mountain-500 Black, 40|Black|40  |27.35 |\n",
      "+------------+---------------+----------------------+-----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flat_query = '''\n",
    "SELECT\n",
    "    pc.Name as CategoryName,\n",
    "    ps.Name as SubCategoryName,\n",
    "    p.Name as ProductName,\n",
    "    p.Color,\n",
    "    p.Size,\n",
    "    p.Weight \n",
    "FROM AdventureWorks.ProductCategory pc \n",
    "INNER JOIN AdventureWorks.ProductSubcategory ps ON pc.ProductCategoryID = ps.ProductCategoryID \n",
    "INNER JOIN AdventureWorks.Product p ON p.ProductSubcategoryID = ps.ProductSubcategoryID \n",
    "'''\n",
    "\n",
    "spark.sql(flat_query).limit(5).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2675044c-ceb1-4b87-8fa7-5a1ac17055bd",
   "metadata": {},
   "source": [
    "## Perform more complex queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bca96fb0-99d2-4de4-833d-e1398ba3ba43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------------+-----------------+\n",
      "|CustomerID|FirstOrderDate     |SecondOrderDate    |DaysBetweenOrders|\n",
      "+----------+-------------------+-------------------+-----------------+\n",
      "|11015     |2013-06-20 00:00:00|2013-06-20 00:00:00|0                |\n",
      "|11016     |2013-07-12 00:00:00|2013-07-12 00:00:00|0                |\n",
      "|11019     |2013-07-15 00:00:00|2013-08-04 00:00:00|20               |\n",
      "|11020     |2013-05-31 00:00:00|2013-05-31 00:00:00|0                |\n",
      "|11021     |2013-06-25 00:00:00|2013-06-25 00:00:00|0                |\n",
      "|11022     |2013-06-22 00:00:00|2013-06-22 00:00:00|0                |\n",
      "|11024     |2013-11-27 00:00:00|2013-12-26 00:00:00|29               |\n",
      "+----------+-------------------+-------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# مشتریانی که بین اولین و دومین سفارششان، کمتر از ۱ ماه طول کشیده باشد\n",
    "\n",
    "query = '''\n",
    "WITH CustomerFirstTwoOrders AS (\n",
    "    SELECT \n",
    "        CustomerID,\n",
    "        OrderDate,\n",
    "        ROW_NUMBER() OVER (PARTITION BY CustomerID ORDER BY OrderDate) AS RowNum\n",
    "    FROM AdventureWorks.SalesOrderHeader\n",
    ")\n",
    "SELECT \n",
    "    c.CustomerID,\n",
    "    MIN(f.OrderDate) AS FirstOrderDate,\n",
    "    MAX(f.OrderDate) AS SecondOrderDate,\n",
    "    DATEDIFF(DAY, MIN(f.OrderDate), MAX(f.OrderDate)) AS DaysBetweenOrders\n",
    "FROM CustomerFirstTwoOrders f\n",
    "INNER JOIN AdventureWorks.Customer c ON f.CustomerID = c.CustomerID\n",
    "WHERE f.RowNum <= 2\n",
    "GROUP BY c.CustomerID\n",
    "HAVING DATEDIFF(DAY, MIN(f.OrderDate), MAX(f.OrderDate)) < 31;\n",
    "'''\n",
    "\n",
    "spark.sql(query).limit(7).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049b6c3a-9689-4f37-ae5c-97dde9e763a2",
   "metadata": {},
   "source": [
    "## Work with files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eedc36cc-cb05-40bc-a0b6-2248d1ea3cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|current_database|\n",
      "+----------------+\n",
      "|otherdb         |\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('USE OtherDB')\n",
    "spark.sql('SELECT current_schema() AS current_database').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "20a710dd-1f6f-4140-9057-b5f7ea6fcf92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+---------+\n",
      "|column_name                |data_type|\n",
      "+---------------------------+---------+\n",
      "|_c0                        |int      |\n",
      "|arrest_key                 |int      |\n",
      "|arrest_date                |date     |\n",
      "|pd_desc                    |string   |\n",
      "|ofns_desc                  |string   |\n",
      "|law_code                   |string   |\n",
      "|law_cat_cd                 |string   |\n",
      "|age_group                  |string   |\n",
      "|perp_sex                   |string   |\n",
      "|perp_race                  |string   |\n",
      "|latitude                   |double   |\n",
      "|longitude                  |double   |\n",
      "|arrest_boro                |string   |\n",
      "|arrest_precinct            |int      |\n",
      "|jurisdiction_code          |double   |\n",
      "|:@computed_region_f5dn_yrer|double   |\n",
      "|:@computed_region_yeji_bk3q|double   |\n",
      "|:@computed_region_92fq_4b7q|double   |\n",
      "|:@computed_region_sbqj_enih|double   |\n",
      "+---------------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crime_df = spark.read.option('header', 'true').option('inferSchema', 'true').option('delimiter', ',').csv('../../Data/crime.csv')\n",
    "spark.createDataFrame(crime_df.dtypes, ['column_name', 'data_type']).show(crime_df.count(), truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1427b68b-b1c5-4de3-b34e-ad52f5592c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------------------------------+------------------------------+---------+--------+---------+------------------+------------------+\n",
      "|arrest_date|pd_desc                                |ofns_desc                     |age_group|perp_sex|perp_race|latitude          |longitude         |\n",
      "+-----------+---------------------------------------+------------------------------+---------+--------+---------+------------------+------------------+\n",
      "|2019-01-26 |SEXUAL ABUSE                           |SEX CRIMES                    |45-64    |M       |BLACK    |40.800694331000045|-73.94110928599997|\n",
      "|2019-02-06 |CRIMINAL SALE OF A CONTROLLED SUBSTANCE|CONTROLLED SUBSTANCES OFFENSES|25-44    |M       |UNKNOWN  |40.75783900300007 |-73.99121211099998|\n",
      "|2016-01-06 |RAPE 3                                 |RAPE                          |25-44    |M       |BLACK    |40.648650085000035|-73.95033556299995|\n",
      "+-----------+---------------------------------------+------------------------------+---------+--------+---------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "    SELECT\n",
    "        arrest_date, pd_desc, ofns_desc, age_group, perp_sex, perp_race, latitude, longitude\n",
    "    FROM {df}\n",
    "    LIMIT 3\n",
    "''', df = crime_df).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ec4bd933-061e-46c3-a566-f8bd32f04bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------------------------------+---------+------+------------------+------------------+\n",
      "|year|month|description                   |age_group|gender|latitude          |longitude         |\n",
      "+----+-----+------------------------------+---------+------+------------------+------------------+\n",
      "|2019|1    |SEX CRIMES                    |45-64    |Male  |40.800694331000045|-73.94110928599997|\n",
      "|2019|2    |CONTROLLED SUBSTANCES OFFENSES|25-44    |Male  |40.75783900300007 |-73.99121211099998|\n",
      "|2016|1    |RAPE                          |25-44    |Male  |40.648650085000035|-73.95033556299995|\n",
      "+----+-----+------------------------------+---------+------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "    SELECT\n",
    "        YEAR(arrest_date) AS year,\n",
    "        MONTH(arrest_date) AS month,\n",
    "        COALESCE(ofns_desc, Null) AS description,\n",
    "        COALESCE(age_group, Null) AS age_group,\n",
    "        CASE\n",
    "            WHEN perp_sex = 'M' THEN 'Male' \n",
    "            WHEN perp_sex = 'F' THEN 'Female' \n",
    "        ELSE Null END AS gender,\n",
    "        latitude,\n",
    "        longitude\n",
    "    FROM {df}\n",
    "    LIMIT 3\n",
    "''', df = crime_df).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a4c89bdf-5139-480a-84d2-3781b03a4585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+\n",
      "|database_name|table_name|\n",
      "+-------------+----------+\n",
      "|otherdb      |crime     |\n",
      "+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = '''\n",
    "CREATE TABLE IF NOT EXISTS crime (\n",
    "    arrest_date DATE,\n",
    "    pd_desc STRING,\n",
    "    ofns_desc STRING,\n",
    "    age_group STRING,\n",
    "    perp_sex STRING,\n",
    "    perp_race STRING,\n",
    "    latitude DOUBLE,\n",
    "    longitude DOUBLE\n",
    ") USING DELTA LOCATION '../lake/bronze/OtherDB/crime'\n",
    "'''\n",
    "\n",
    "spark.sql(sql)\n",
    "spark.sql('SELECT namespace as database_name, tableName as table_name FROM {df}', df = spark.sql('SHOW TABLES IN OtherDB')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6b8b5b7c-8197-429d-b06f-3b8a7d279aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|crime_row_count|\n",
      "+---------------+\n",
      "|3881989        |\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "    INSERT INTO crime\n",
    "    SELECT arrest_date, pd_desc, ofns_desc, age_group, perp_sex, perp_race, latitude, longitude FROM {df} ''', df = crime_df)\n",
    "\n",
    "spark.sql('SELECT COUNT(*) AS crime_row_count FROM crime').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "10320460-681e-4552-a26d-496285adb0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+\n",
      "|database_name|view_name|\n",
      "+-------------+---------+\n",
      "|otherdb      |vw_crime |\n",
      "+-------------+---------+\n",
      "\n",
      "+----+-----+------------------------------+---------+------+------------------+------------------+\n",
      "|year|month|description                   |age_group|gender|latitude          |longitude         |\n",
      "+----+-----+------------------------------+---------+------+------------------+------------------+\n",
      "|2019|1    |SEX CRIMES                    |45-64    |Male  |40.800694331000045|-73.94110928599997|\n",
      "|2019|2    |CONTROLLED SUBSTANCES OFFENSES|25-44    |Male  |40.75783900300007 |-73.99121211099998|\n",
      "|2016|1    |RAPE                          |25-44    |Male  |40.648650085000035|-73.95033556299995|\n",
      "|2018|11   |RAPE                          |25-44    |Male  |40.67458330800008 |-73.93022154099998|\n",
      "+----+-----+------------------------------+---------+------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = '''\n",
    "CREATE OR REPLACE VIEW vw_crime AS\n",
    "    SELECT\n",
    "        YEAR(arrest_date) AS year,\n",
    "        MONTH(arrest_date) AS month,\n",
    "        COALESCE(ofns_desc, Null) AS description,\n",
    "        COALESCE(age_group, Null) AS age_group,\n",
    "        CASE \n",
    "            WHEN perp_sex = 'M' THEN 'Male' \n",
    "            WHEN perp_sex = 'F' THEN 'Female' \n",
    "        ELSE Null END AS gender,\n",
    "        latitude,\n",
    "        longitude\n",
    "    FROM crime\n",
    "'''\n",
    "\n",
    "spark.sql(sql)\n",
    "spark.sql('SELECT namespace as database_name, viewName as view_name FROM {df}', df = spark.sql('SHOW VIEWS IN OtherDB')).show(truncate=False)\n",
    "spark.sql('SELECT * FROM vw_crime LIMIT 4').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5762765f-0ca4-40a4-b34b-b3eb567488a3",
   "metadata": {},
   "source": [
    "## Query all files inside a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fca4d2c5-e418-42be-b228-887c65565d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_df = spark.read.parquet('../../Data/Trip/*/*.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bb5f515e-9233-4284-8efa-ebb24dfa4ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|count_rides|\n",
      "+-----------+\n",
      "|565621687  |\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT COUNT(*) count_rides FROM {df}', df = trip_df).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "04f9ac11-995f-4ba0-8a18-e1ec4395c02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+---------------+-------------+------------+-----------+-------+----------+------------+------------+\n",
      "|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|payment_type|fare_amount|mta_tax|tip_amount|tolls_amount|total_amount|\n",
      "+--------------------+---------------------+---------------+-------------+------------+-----------+-------+----------+------------+------------+\n",
      "|2016-01-01 00:12:22 |2016-01-01 00:29:14  |1              |3.2          |1           |14.0       |0.5    |3.06      |0.0         |18.36       |\n",
      "|2016-01-01 00:41:31 |2016-01-01 00:55:10  |2              |1.0          |2           |9.5        |0.5    |0.0       |0.0         |10.8        |\n",
      "|2016-01-01 00:53:37 |2016-01-01 00:59:57  |1              |0.9          |2           |6.0        |0.5    |0.0       |0.0         |7.3         |\n",
      "+--------------------+---------------------+---------------+-------------+------------+-----------+-------+----------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "    SELECT\n",
    "        tpep_pickup_datetime, tpep_dropoff_datetime, passenger_count, trip_distance, payment_type, fare_amount, mta_tax, tip_amount, tolls_amount, total_amount\n",
    "    FROM {df}\n",
    "    LIMIT 3\n",
    "''', df = trip_df).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96632d44-8b8a-40ce-8000-b54d606412ea",
   "metadata": {},
   "source": [
    "## Use DuckDB to safely bring multiple files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d4049705-33b8-46e1-b58f-7fc679ae1e8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_row_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>565621687</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   trip_row_count\n",
       "0       565621687"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import duckdb\n",
    "conn = duckdb.connect('../temp/tempdb.duckdb')\n",
    "conn.sql('''\n",
    "    SELECT COUNT(*) AS trip_row_count FROM read_parquet('../../Data/Trip/*/*.parquet')\n",
    "''').df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ddc1d0b3-8c71-46a8-8e89-1c5717055437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3082e31b1606484a9c40ccddddf6ac27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sql = '''\n",
    "CREATE OR REPLACE TABLE trip AS\n",
    "    SELECT\n",
    "        tpep_pickup_datetime,\n",
    "        tpep_dropoff_datetime,\n",
    "        passenger_count,\n",
    "        trip_distance,\n",
    "        payment_type,\n",
    "        fare_amount,\n",
    "        mta_tax,\n",
    "        tip_amount,\n",
    "        tolls_amount,\n",
    "        total_amount\n",
    "    FROM read_parquet('../../Data/Trip/*/*.parquet')\n",
    "'''\n",
    "\n",
    "conn.sql(sql)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a1e58e17-af85-479a-9b22-5fc274e5cdba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e600c20624b94c7b9f43835d1a896a4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conn = duckdb.connect('../temp/tempdb.duckdb')\n",
    "conn.sql('COPY trip TO \"../temp/trip.parquet\" (FORMAT PARQUET)')\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "87047535-2512-4645-8c2f-ae887ccc9b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-------------+\n",
      "|column_name          |data_type    |\n",
      "+---------------------+-------------+\n",
      "|tpep_pickup_datetime |timestamp_ntz|\n",
      "|tpep_dropoff_datetime|timestamp_ntz|\n",
      "|passenger_count      |bigint       |\n",
      "|trip_distance        |double       |\n",
      "|payment_type         |bigint       |\n",
      "|fare_amount          |double       |\n",
      "|mta_tax              |double       |\n",
      "|tip_amount           |double       |\n",
      "|tolls_amount         |double       |\n",
      "|total_amount         |double       |\n",
      "+---------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trip_df = spark.read.parquet('../temp/trip.parquet')\n",
    "spark.createDataFrame(trip_df.dtypes, ['column_name', 'data_type']).show(trip_df.count(), truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b62559fb-3fc2-4fac-970b-8af124cbdfe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+\n",
      "|database_name|table_name|\n",
      "+-------------+----------+\n",
      "|otherdb      |crime     |\n",
      "|otherdb      |trip      |\n",
      "|otherdb      |vw_crime  |\n",
      "+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# partition table for huge amount of data\n",
    "\n",
    "sql = '''\n",
    "CREATE TABLE IF NOT EXISTS trip (\n",
    "    tpep_pickup_datetime TIMESTAMP_NTZ,\n",
    "    tpep_dropoff_datetime TIMESTAMP_NTZ,\n",
    "    passenger_count BIGINT,\n",
    "    trip_distance DOUBLE,\n",
    "    payment_type BIGINT,\n",
    "    fare_amount DOUBLE,\n",
    "    mta_tax DOUBLE,\n",
    "    tip_amount DOUBLE,\n",
    "    tolls_amount DOUBLE,\n",
    "    total_amount DOUBLE\n",
    ") USING DELTA LOCATION '../lake/bronze/OtherDB/trip' PARTITIONED BY (payment_type)\n",
    "'''\n",
    "\n",
    "spark.sql(sql)\n",
    "spark.sql('SELECT namespace as database_name, tableName as table_name FROM {df}', df = spark.sql('SHOW TABLES IN OtherDB')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "563527a8-7fd9-41c7-97c3-bc9d1d86cfee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|trip_row_count|\n",
      "+--------------+\n",
      "|565621687     |\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "    INSERT INTO trip\n",
    "    SELECT\n",
    "        tpep_pickup_datetime, tpep_dropoff_datetime, passenger_count, trip_distance, payment_type, fare_amount, mta_tax, tip_amount, tolls_amount, total_amount\n",
    "    FROM {df}\n",
    "''', df = trip_df)\n",
    "\n",
    "spark.sql('SELECT COUNT(*) AS trip_row_count FROM OtherDB.trip').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "41c56b7f-ace7-498b-8850-934e03d8a59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -r ../temp/trip.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b8481d-06bf-47b9-8b2f-b51d98b5885d",
   "metadata": {},
   "source": [
    "## Fast query a delta table using DuckDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9fdf936f-4b67-4a76-a1c6-8af9597a5492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>extension_name</th>\n",
       "      <th>installed</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>delta</td>\n",
       "      <td>True</td>\n",
       "      <td>Adds support for Delta Lake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  extension_name  installed                  description\n",
       "0          delta       True  Adds support for Delta Lake"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import duckdb\n",
    "duckdb.sql('''SELECT extension_name, installed, description FROM duckdb_extensions() WHERE extension_name='delta' ''').df()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4e8868f6-f618-492c-94a4-b4e616c53690",
   "metadata": {},
   "source": [
    "duckdb.sql('INSTALL DELTA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b476c4af-5255-4219-adea-3dbf2a53833d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eded218efa964c45a549453a94447973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Payment type</th>\n",
       "      <th>Passengers Average</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.548711</td>\n",
       "      <td>7.722359e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.625162</td>\n",
       "      <td>2.371349e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.741566e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.261405</td>\n",
       "      <td>5.061590e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.305835</td>\n",
       "      <td>1.085476e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>1.116883</td>\n",
       "      <td>7.982000e+02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Payment type  Passengers Average        Amount\n",
       "0             1            1.548711  7.722359e+09\n",
       "1             2            1.625162  2.371349e+09\n",
       "2             0            0.000000  1.741566e+08\n",
       "3             3            1.261405  5.061590e+07\n",
       "4             4            1.305835  1.085476e+07\n",
       "5             5            1.116883  7.982000e+02"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duckdb.sql('LOAD DELTA')\n",
    "\n",
    "sql = '''\n",
    "SELECT\n",
    "    payment_type AS 'Payment type',\n",
    "    COALESCE(AVG(passenger_count), 0) AS 'Passengers Average',\n",
    "    COALESCE(SUM(total_amount), 0) AS Amount\n",
    "FROM delta_scan('../lake/bronze/OtherDB/trip/')\n",
    "GROUP BY payment_type\n",
    "ORDER BY 3 DESC, 2 DESC\n",
    "'''\n",
    "\n",
    "duckdb.sql(sql).df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e4d20a85-d793-441b-bae6-ca14718fd10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------+\n",
      "|database_name|table_name      |\n",
      "+-------------+----------------+\n",
      "|otherdb      |crime           |\n",
      "|otherdb      |trip            |\n",
      "|otherdb      |vw_crime        |\n",
      "|otherdb      |vw_trip_duration|\n",
      "+-------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = '''\n",
    "CREATE OR REPLACE VIEW vw_trip_duration AS\n",
    "    SELECT\n",
    "        total_amount as amount,\n",
    "        UNIX_TIMESTAMP(tpep_dropoff_datetime) - UNIX_TIMESTAMP(tpep_pickup_datetime) AS duration,\n",
    "        trip_distance as distance,\n",
    "        hour(tpep_pickup_datetime) as hour,\n",
    "        CAST(passenger_count AS INT) AS passenger,\n",
    "        payment_type as payment\n",
    "    FROM trip\n",
    "'''\n",
    "\n",
    "spark.sql(sql)\n",
    "spark.sql('SELECT namespace as database_name, tableName as table_name FROM {df}', df = spark.sql('SHOW TABLES IN OtherDB')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e538e950-8fa4-4b88-9ed4-71dfec71d53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+--------+----+---------+-------+\n",
      "|amount|duration|distance|hour|passenger|payment|\n",
      "+------+--------+--------+----+---------+-------+\n",
      "|8.76  |321     |0.87    |18  |1        |1      |\n",
      "|9.96  |542     |0.84    |17  |1        |1      |\n",
      "|7.8   |289     |0.7     |18  |1        |1      |\n",
      "+------+--------+--------+----+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM vw_trip_duration LIMIT 3').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb315fe-f907-48d9-8183-00f3a7e80f97",
   "metadata": {},
   "source": [
    "## Create view for reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3aec44b7-afce-4874-a608-96d52ca7bf27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------+\n",
      "|database_name|view_name       |\n",
      "+-------------+----------------+\n",
      "|otherdb      |vw_crime        |\n",
      "|otherdb      |vw_trip_duration|\n",
      "|otherdb      |vw_trip_report  |\n",
      "+-------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = '''\n",
    "CREATE OR REPLACE VIEW vw_trip_report AS\n",
    "    WITH trip AS (\n",
    "        SELECT\n",
    "            MONTH(tpep_pickup_datetime) AS month,\n",
    "            CAST(passenger_count AS INT) AS passenger_count,\n",
    "            payment_type,\n",
    "            UNIX_TIMESTAMP(tpep_dropoff_datetime) - UNIX_TIMESTAMP(tpep_pickup_datetime) AS duration\n",
    "    FROM trip\n",
    "    )\n",
    "    SELECT\n",
    "        CASE month\n",
    "            WHEN 1 THEN 'January'\n",
    "            WHEN 2 THEN 'February'\n",
    "            WHEN 3 THEN 'March'\n",
    "            WHEN 4 THEN 'April'\n",
    "            WHEN 5 THEN 'May'\n",
    "            WHEN 6 THEN 'June'\n",
    "            WHEN 7 THEN 'July'\n",
    "            WHEN 8 THEN 'August'\n",
    "            WHEN 9 THEN 'September'\n",
    "            WHEN 10 THEN 'October'\n",
    "            WHEN 11 THEN 'November'\n",
    "            WHEN 12 THEN 'December'\n",
    "        END AS month,\n",
    "        CASE payment_type\n",
    "            WHEN 0 THEN 'Cash'\n",
    "            WHEN 1 THEN 'Credit Card'\n",
    "            WHEN 2 THEN 'Debit Card'\n",
    "            WHEN 3 THEN 'Free of Charge'\n",
    "            ELSE 'Unknown'\n",
    "        END AS payment_type,\n",
    "        COALESCE(\n",
    "            SUM(passenger_count), 0\n",
    "        ) AS total_passenger_count\n",
    "    FROM trip\n",
    "    GROUP BY month, payment_type\n",
    "    ORDER BY total_passenger_count DESC\n",
    "'''\n",
    "\n",
    "spark.sql(sql)\n",
    "spark.sql('SELECT namespace as database_name, viewName as view_name FROM {df}', df = spark.sql('SHOW VIEWS IN OtherDB')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9ffd9a10-17ce-43bd-b0ba-d37b59986485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+---------------------+\n",
      "|month    |payment_type|total_passenger_count|\n",
      "+---------+------------+---------------------+\n",
      "|March    |Credit Card |56547957             |\n",
      "|February |Credit Card |55005683             |\n",
      "|January  |Credit Card |54953193             |\n",
      "|October  |Credit Card |52828619             |\n",
      "|May      |Credit Card |52607317             |\n",
      "|April    |Credit Card |52167256             |\n",
      "|June     |Credit Card |50495234             |\n",
      "|November |Credit Card |49611323             |\n",
      "|December |Credit Card |49373428             |\n",
      "|September|Credit Card |47257834             |\n",
      "+---------+------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM vw_trip_report LIMIT 10').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2737893-243d-4e30-979d-e788f9a5e501",
   "metadata": {},
   "source": [
    "## Data Quality check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0da7d96b-a48f-4303-a416-dc1ea0fab3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_df = spark.sql('SELECT * FROM OtherDB.trip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d2fb458a-5b8b-48b4-9bb2-f4280edee1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------------+-------------------+--------------------+\n",
      "|entity     |instance               |name               |value               |\n",
      "+-----------+-----------------------+-------------------+--------------------+\n",
      "|Multicolumn|payment_type,tip_amount|Correlation        |-3.51810113525617E-4|\n",
      "|Column     |tpep_pickup_datetime   |Completeness       |1.0                 |\n",
      "|Column     |more-5 passenger_count |Compliance         |0.06527827848298186 |\n",
      "|Dataset    |*                      |Size               |5.65621687E8        |\n",
      "|Column     |payment_type           |ApproxCountDistinct|6.0                 |\n",
      "|Column     |passenger_count        |Mean               |1.5685495143314883  |\n",
      "|Column     |less-0 passenger_count |Compliance         |0.009154656051934586|\n",
      "+-----------+-----------------------+-------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/morteza/Documents/Spark/venv/lib/python3.12/site-packages/pyspark/sql/dataframe.py:147: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
     ]
    }
   ],
   "source": [
    "analysisResult = AnalysisRunner(spark) \\\n",
    "                    .onData(trip_df) \\\n",
    "                    .addAnalyzer(Size()) \\\n",
    "                    .addAnalyzer(Completeness('tpep_pickup_datetime')) \\\n",
    "                    .addAnalyzer(ApproxCountDistinct('payment_type')) \\\n",
    "                    .addAnalyzer(Mean('passenger_count')) \\\n",
    "                    .addAnalyzer(Correlation('payment_type', 'tip_amount')) \\\n",
    "                    .addAnalyzer(Compliance('more-5 passenger_count', 'passenger_count >= 5')) \\\n",
    "                    .addAnalyzer(Compliance('less-0 passenger_count', 'passenger_count <= 0')) \\\n",
    "                    .run()\n",
    "                    \n",
    "analysisResult_df = AnalyzerContext.successMetricsAsDataFrame(spark, analysisResult)\n",
    "analysisResult_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2563b7a9-4d0d-4ef7-a05e-c4be0bc1cded",
   "metadata": {},
   "source": [
    "## Silver zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4fba3b0f-aeed-4b9d-ac8d-3370ea03b153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+---------------+-----+---------+----+------+-----------------------+\n",
      "|ProductID|ProductSubcategoryID|Name           |Color|ListPrice|Size|Weight|ModifiedDate           |\n",
      "+---------+--------------------+---------------+-----+---------+----+------+-----------------------+\n",
      "|1        |NULL                |Adjustable Race|NULL |0.0000   |NULL|NULL  |2014-02-08 10:01:36.827|\n",
      "+---------+--------------------+---------------+-----+---------+----+------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM AdventureWorks.Product LIMIT 1').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f238ff67-a7cf-44c3-abb0-85f96a11b68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+--------------+-------------------+\n",
      "|ProductSubcategoryID|ProductCategoryID|Name          |ModifiedDate       |\n",
      "+--------------------+-----------------+--------------+-------------------+\n",
      "|1                   |1                |Mountain Bikes|2008-04-30 00:00:00|\n",
      "+--------------------+-----------------+--------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM AdventureWorks.ProductSubCategory LIMIT 1').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b0a328e3-716f-41d7-834f-a5ebe1263672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+-------------------+\n",
      "|ProductCategoryID|Name |ModifiedDate       |\n",
      "+-----------------+-----+-------------------+\n",
      "|1                |Bikes|2008-04-30 00:00:00|\n",
      "+-----------------+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM AdventureWorks.ProductCategory LIMIT 1').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e67e1683-51d4-4a25-aeff-341905df6498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|namespace     |\n",
      "+--------------+\n",
      "|adventureworks|\n",
      "|default       |\n",
      "|otherdb       |\n",
      "|warehouse     |\n",
      "+--------------+\n",
      "\n",
      "+----------------+\n",
      "|current_database|\n",
      "+----------------+\n",
      "|otherdb         |\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('CREATE DATABASE IF NOT EXISTS Warehouse')\n",
    "spark.sql('SHOW DATABASES').show(truncate=False)\n",
    "spark.sql('SELECT current_schema() AS current_database').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e191fe1e-31ac-4480-a93a-ecee4ef039f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "SELECT\n",
    "    p.ProductID,\n",
    "    p.Name AS ProductName,\n",
    "    p.Color,\n",
    "    p.Size,\n",
    "    p.Weight,\n",
    "    sc.Name AS SubCategory,\n",
    "    c.Name AS Category\n",
    "FROM AdventureWorks.Product p\n",
    "LEFT JOIN AdventureWorks.ProductSubCategory sc ON p.ProductSubcategoryID = sc.ProductSubcategoryID\n",
    "LEFT JOIN AdventureWorks.ProductCategory c ON sc.ProductCategoryID = c.ProductCategoryID\n",
    "'''\n",
    "\n",
    "data = spark.sql(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3212de86-1f88-4415-af16-5fe9f34ae4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+\n",
      "|database_name|table_name|\n",
      "+-------------+----------+\n",
      "|warehouse    |dimproduct|\n",
      "+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = '''\n",
    "CREATE TABLE IF NOT EXISTS Warehouse.DimProduct (\n",
    "    Sk INT NOT NULL,\n",
    "    ProductID INT,\n",
    "    ProductName STRING,\n",
    "    Color STRING,\n",
    "    Size STRING,\n",
    "    Weight DECIMAL(8,2),\n",
    "    SubCategory STRING,\n",
    "    Category STRING,\n",
    "    StartDate TIMESTAMP,\n",
    "    EndDate TIMESTAMP\n",
    ") USING DELTA LOCATION '../lake/silver/Warehouse/DimProduct'\n",
    "'''\n",
    "\n",
    "spark.sql(sql)\n",
    "spark.sql('SELECT namespace as database_name, tableName as table_name FROM {df}', df = spark.sql('SHOW TABLES IN Warehouse')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b3b16b60-a5a7-4708-8579-9a87f6f5e61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---------------+-----+----+------+-----------+--------+--------------------------+-------+\n",
      "|Sk |ProductID|ProductName    |Color|Size|Weight|SubCategory|Category|StartDate                 |EndDate|\n",
      "+---+---------+---------------+-----+----+------+-----------+--------+--------------------------+-------+\n",
      "|1  |1        |Adjustable Race|NULL |NULL|NULL  |NULL       |NULL    |2024-10-29 11:31:22.268866|NULL   |\n",
      "|2  |2        |Bearing Ball   |NULL |NULL|NULL  |NULL       |NULL    |2024-10-29 11:31:22.268866|NULL   |\n",
      "|3  |3        |BB Ball Bearing|NULL |NULL|NULL  |NULL       |NULL    |2024-10-29 11:31:22.268866|NULL   |\n",
      "+---+---------+---------------+-----+----+------+-----------+--------+--------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create dimension with SCD type 2\n",
    "\n",
    "sql = '''\n",
    "INSERT INTO Warehouse.DimProduct\n",
    "SELECT\n",
    "    ROW_NUMBER() OVER(ORDER BY NULL) + (SELECT COALESCE(MAX(Sk), 0) FROM Warehouse.DimProduct) AS Sk,\n",
    "    *,\n",
    "    CURRENT_TIMESTAMP() AS StartDate, \n",
    "    NULL as EndDate\n",
    "FROM {df}\n",
    "'''\n",
    "\n",
    "spark.sql(sql, df = data)\n",
    "spark.sql('SELECT * FROM Warehouse.DimProduct LIMIT 3').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "600eb0d6-e6b3-4fef-bce0-cc2cc1062eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create fact order\n",
    "\n",
    "sql = '''\n",
    "SELECT\n",
    "    soh.SalesOrderID AS OrderId,\n",
    "    sod.SalesOrderDetailID AS OrderDetailsId,\n",
    "    CAST(soh.OrderDate AS DATE) AS OrderDate,\n",
    "    soh.CustomerID AS CustomerId,\n",
    "    sod.ProductID AS ProductId,\n",
    "    sod.OrderQty AS Quantity,\n",
    "    sod.UnitPrice AS Price\n",
    "FROM AdventureWorks.SalesOrderHeader soh\n",
    "LEFT JOIN AdventureWorks.SalesOrderDetail sod ON soh.SalesOrderID = sod.SalesOrderID\n",
    "'''\n",
    "\n",
    "data = spark.sql(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fe5ba954-640a-40f2-95c9-4ee2d07c5d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+\n",
      "|database_name|table_name|\n",
      "+-------------+----------+\n",
      "|warehouse    |dimproduct|\n",
      "|warehouse    |factorder |\n",
      "+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = '''\n",
    "CREATE TABLE IF NOT EXISTS Warehouse.FactOrder (\n",
    "    OrderId INT,\n",
    "    OrderDetailsId INT,\n",
    "    OrderDate DATE,\n",
    "    CustomerId INT,\n",
    "    ProductId INT,\n",
    "    Quantity SMALLINT,\n",
    "    Price DECIMAL(19,4)\n",
    ") USING DELTA LOCATION '../lake/silver/Warehouse/FactOrder'\n",
    "'''\n",
    "\n",
    "spark.sql(sql)\n",
    "spark.sql('SELECT namespace as database_name, tableName as table_name FROM {df}', df = spark.sql('SHOW TABLES IN Warehouse')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f9377d7b-a6b4-4e80-9614-50ee8bb00bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+----------+----------+---------+--------+-------+\n",
      "|OrderId|OrderDetailsId|OrderDate |CustomerId|ProductId|Quantity|Price  |\n",
      "+-------+--------------+----------+----------+---------+--------+-------+\n",
      "|43659  |12            |2011-05-31|29825     |711      |4       |20.1865|\n",
      "|43659  |11            |2011-05-31|29825     |712      |2       |5.1865 |\n",
      "|43659  |10            |2011-05-31|29825     |709      |6       |5.7000 |\n",
      "+-------+--------------+----------+----------+---------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "    INSERT INTO Warehouse.FactOrder\n",
    "    SELECT * FROM {df}\n",
    "''', df = data)\n",
    "\n",
    "spark.sql('SELECT * FROM Warehouse.FactOrder LIMIT 3').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "403fb4fe-992c-47ad-ba82-90c0b00468d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+\n",
      "|min(OrderDate)|max(OrderDate)|\n",
      "+--------------+--------------+\n",
      "|2011-05-31    |2014-06-30    |\n",
      "+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create dimension date\n",
    "\n",
    "spark.sql('SELECT MIN(OrderDate), MAX(OrderDate) FROM Warehouse.FactOrder').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "061defd8-3008-4c78-a152-ff5c9e61213d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "sql = '''\n",
    "SELECT \n",
    "    date_key AS DateKey,\n",
    "    YEAR(date_key) AS Year,\n",
    "    MONTH(date_key) AS MonthKey,\n",
    "    MONTHNAME(date_key) AS MonthName,\n",
    "    DAY(date_key) AS Day,\n",
    "    DAYNAME(date_key) AS DayName,\n",
    "    DAYOFYEAR(date_key) AS DayOfYear,\n",
    "    DAYOFMONTH(date_key) AS DayOfMonth,\n",
    "    QUARTER(date_key) AS Quarter\n",
    "FROM (\n",
    "        SELECT CAST(range AS DATE) AS date_key FROM RANGE(DATE '2011-05-30', DATE '2014-06-29', INTERVAL 1 DAY)\n",
    ") q\n",
    "'''\n",
    "\n",
    "data = spark.createDataFrame( \n",
    "    duckdb.sql(sql).df()\n",
    ")\n",
    "\n",
    "data = data.withColumn('DateKey', col('DateKey').cast('date'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "55bafcec-18c4-4976-9844-1462195600a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+\n",
      "|database_name|table_name|\n",
      "+-------------+----------+\n",
      "|warehouse    |dimdate   |\n",
      "|warehouse    |dimproduct|\n",
      "|warehouse    |factorder |\n",
      "+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = '''\n",
    "CREATE TABLE IF NOT EXISTS Warehouse.DimDate (\n",
    "    Sk INT NOT NULL,\n",
    "    DateKey DATE,\n",
    "    Year BIGINT,\n",
    "    MonthKey BIGINT,\n",
    "    MonthName STRING,\n",
    "    Day BIGINT,\n",
    "    DayName STRING,\n",
    "    DayOfYear BIGINT,\n",
    "    DayOfMonth BIGINT,\n",
    "    Quarter BIGINT,\n",
    "    StartDate TIMESTAMP,\n",
    "    EndDate TIMESTAMP\n",
    ") USING DELTA LOCATION '../lake/silver/Warehouse/DimDate'\n",
    "'''\n",
    "\n",
    "spark.sql(sql)\n",
    "spark.sql('SELECT namespace as database_name, tableName as table_name FROM {df}', df = spark.sql('SHOW TABLES IN Warehouse')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4344c60e-8686-49ed-b81e-df8f5dd344e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----+--------+---------+---+---------+---------+----------+-------+--------------------------+-------+\n",
      "|Sk |DateKey   |Year|MonthKey|MonthName|Day|DayName  |DayOfYear|DayOfMonth|Quarter|StartDate                 |EndDate|\n",
      "+---+----------+----+--------+---------+---+---------+---------+----------+-------+--------------------------+-------+\n",
      "|1  |2011-05-30|2011|5       |May      |30 |Monday   |150      |30        |2      |2024-10-29 11:31:44.336643|NULL   |\n",
      "|2  |2011-05-31|2011|5       |May      |31 |Tuesday  |151      |31        |2      |2024-10-29 11:31:44.336643|NULL   |\n",
      "|3  |2011-06-01|2011|6       |June     |1  |Wednesday|152      |1         |2      |2024-10-29 11:31:44.336643|NULL   |\n",
      "+---+----------+----+--------+---------+---+---------+---------+----------+-------+--------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = '''\n",
    "INSERT INTO Warehouse.DimDate\n",
    "SELECT\n",
    "    ROW_NUMBER() OVER(ORDER BY NULL) + (SELECT COALESCE(MAX(Sk), 0) FROM Warehouse.DimDate) AS Sk,\n",
    "    *,\n",
    "    CURRENT_TIMESTAMP() AS StartDate, \n",
    "    NULL as EndDate\n",
    "FROM {df}\n",
    "'''\n",
    "\n",
    "spark.sql(sql, df = data)\n",
    "spark.sql('SELECT * FROM Warehouse.DimDate LIMIT 3').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9a9da2e0-5e5e-411f-856f-08ae16087ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dimension customer\n",
    "\n",
    "sql = '''\n",
    "SELECT\n",
    "\tc.CustomerID,\n",
    "\tCONCAT(\n",
    "\t\tCOALESCE(p.FirstName, ''), ' ',\n",
    "\t\tCOALESCE(p.MiddleName, ''), ' ',\n",
    "\t\tCOALESCE (p.LastName, '')\n",
    "\t) AS FullName,\n",
    "\tea.EmailAddress,\n",
    "\tpp.PhoneNumber \n",
    "FROM AdventureWorks.Customer c\n",
    "INNER JOIN AdventureWorks.Person p ON c.PersonID = p.BusinessEntityID\n",
    "INNER JOIN AdventureWorks.BusinessEntity be ON be.BusinessEntityID = p.BusinessEntityID \n",
    "INNER JOIN AdventureWorks.EmailAddress ea ON ea.BusinessEntityID = be.BusinessEntityID \n",
    "INNER JOIN AdventureWorks.PersonPhone pp ON pp.BusinessEntityID = be.BusinessEntityID \n",
    "'''\n",
    "\n",
    "data = spark.sql(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f18e986c-bd9a-4d1e-b265-e7c961703a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+\n",
      "|database_name|table_name |\n",
      "+-------------+-----------+\n",
      "|warehouse    |dimcustomer|\n",
      "|warehouse    |dimdate    |\n",
      "|warehouse    |dimproduct |\n",
      "|warehouse    |factorder  |\n",
      "+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = '''\n",
    "CREATE TABLE IF NOT EXISTS Warehouse.DimCustomer (\n",
    "    Sk INT NOT NULL,\n",
    "    CustomerID INT,\n",
    "    FullName STRING,\n",
    "    EmailAddress STRING,\n",
    "    PhoneNumber STRING,\n",
    "    StartDate TIMESTAMP,\n",
    "    EndDate TIMESTAMP\n",
    ") USING DELTA LOCATION '../lake/silver/Warehouse/DimCustomer'\n",
    "'''\n",
    "\n",
    "spark.sql(sql)\n",
    "spark.sql('SELECT namespace as database_name, tableName as table_name FROM {df}', df = spark.sql('SHOW TABLES IN Warehouse')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a4e664d7-c4e2-4389-8270-77ff43ed5398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------------+------------------------------+------------+--------------------------+-------+\n",
      "|Sk |CustomerID|FullName         |EmailAddress                  |PhoneNumber |StartDate                 |EndDate|\n",
      "+---+----------+-----------------+------------------------------+------------+--------------------------+-------+\n",
      "|1  |29484     |Gustavo  Achong  |gustavo0@adventure-works.com  |398-555-0132|2024-10-29 11:31:50.429922|NULL   |\n",
      "|2  |29485     |Catherine R. Abel|catherine0@adventure-works.com|747-555-0171|2024-10-29 11:31:50.429922|NULL   |\n",
      "|3  |29486     |Kim  Abercrombie |kim2@adventure-works.com      |334-555-0137|2024-10-29 11:31:50.429922|NULL   |\n",
      "+---+----------+-----------------+------------------------------+------------+--------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = '''\n",
    "INSERT INTO Warehouse.DimCustomer\n",
    "SELECT\n",
    "    ROW_NUMBER() OVER(ORDER BY NULL) + (SELECT COALESCE(MAX(Sk), 0) FROM Warehouse.DimCustomer) AS Sk,\n",
    "    *,\n",
    "    CURRENT_TIMESTAMP() AS StartDate, \n",
    "    NULL AS EndDate\n",
    "FROM {df}\n",
    "'''\n",
    "\n",
    "spark.sql(sql, df = data)\n",
    "spark.sql('SELECT * FROM Warehouse.DimCustomer LIMIT 3').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81d767a-6751-409a-b20d-1aac5b41056e",
   "metadata": {},
   "source": [
    "## Farsi data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "45467331-09b0-4985-ae64-04c4764698fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+---------+\n",
      "|column_name          |data_type|\n",
      "+---------------------+---------+\n",
      "|ID_Order             |int      |\n",
      "|ID_Customer          |int      |\n",
      "|ID_Item              |int      |\n",
      "|DateTime_CartFinalize|timestamp|\n",
      "|Amount_Gross_Order   |double   |\n",
      "|city_name_fa         |string   |\n",
      "|Quantity_item        |double   |\n",
      "+---------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "digikala_df = spark.read.option('header', 'true').option('inferSchema', 'true').option('delimiter', ',').csv('../../Data/Digikala/digikala_orders.csv')\n",
    "spark.createDataFrame(digikala_df.dtypes, ['column_name', 'data_type']).show(digikala_df.count(), truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7a0e51a9-d5b9-4634-928c-a9ad60252748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>ID_Order</th><th>ID_Customer</th><th>ID_Item</th><th>DateTime_CartFinalize</th><th>Amount_Gross_Order</th><th>city_name_fa</th><th>Quantity_item</th></tr>\n",
       "<tr><td>2714054</td><td>469662</td><td>21386</td><td>2015-10-15 08:50:56</td><td>597982.0</td><td>محمود آباد</td><td>1.0</td></tr>\n",
       "<tr><td>11104039</td><td>3063877</td><td>248497</td><td>2018-02-11 00:29:26</td><td>980000.0</td><td>خرمدره</td><td>1.0</td></tr>\n",
       "<tr><td>4228130</td><td>3184893</td><td>50144</td><td>2016-06-14 00:30:08</td><td>229358.0</td><td>قرچک</td><td>1.0</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+-----------+-------+---------------------+------------------+------------+-------------+\n",
       "|ID_Order|ID_Customer|ID_Item|DateTime_CartFinalize|Amount_Gross_Order|city_name_fa|Quantity_item|\n",
       "+--------+-----------+-------+---------------------+------------------+------------+-------------+\n",
       "| 2714054|     469662|  21386|  2015-10-15 08:50:56|          597982.0|  محمود آباد|          1.0|\n",
       "|11104039|    3063877| 248497|  2018-02-11 00:29:26|          980000.0|      خرمدره|          1.0|\n",
       "| 4228130|    3184893|  50144|  2016-06-14 00:30:08|          229358.0|        قرچک|          1.0|\n",
       "+--------+-----------+-------+---------------------+------------------+------------+-------------+"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('''\n",
    "    SELECT * FROM {df} LIMIT 3\n",
    "''', df = digikala_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9fcc0798-96b7-4045-b4f7-a8e2b48489fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------+\n",
      "|database_name|table_name      |\n",
      "+-------------+----------------+\n",
      "|otherdb      |crime           |\n",
      "|otherdb      |dg_orders       |\n",
      "|otherdb      |trip            |\n",
      "|otherdb      |vw_crime        |\n",
      "|otherdb      |vw_trip_duration|\n",
      "|otherdb      |vw_trip_report  |\n",
      "+-------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql = '''\n",
    "CREATE TABLE IF NOT EXISTS OtherDB.dg_Orders (\n",
    "    OrderID INT,\n",
    "    CustomerID INT,\n",
    "    ProductID INT,\n",
    "    OrderDate TIMESTAMP,\n",
    "    Amount DOUBLE,\n",
    "    Quantity DOUBLE,\n",
    "    CityName STRING\n",
    ") USING DELTA LOCATION '../lake/bronze/OtherDB/dg_orders'\n",
    "'''\n",
    "\n",
    "\n",
    "spark.sql(sql)\n",
    "spark.sql('SELECT namespace as database_name, tableName as table_name FROM {df}', df = spark.sql('SHOW TABLES IN OtherDB')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2587f90e-e5cc-4d34-9445-806133394516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>OrderID</th><th>CustomerID</th><th>ProductID</th><th>OrderDate</th><th>Amount</th><th>Quantity</th><th>CityName</th></tr>\n",
       "<tr><td>1496884</td><td>468717</td><td>20187</td><td>2014-11-05 16:05:03</td><td>2844074.0</td><td>1.0</td><td>کرج</td></tr>\n",
       "<tr><td>5958461</td><td>2397262</td><td>78180</td><td>2017-01-02 22:03:04</td><td>369450.0</td><td>1.0</td><td>کرمانشاه</td></tr>\n",
       "<tr><td>6345216</td><td>1040870</td><td>273295</td><td>2017-02-11 01:20:22</td><td>156789.0</td><td>1.0</td><td>قزوین</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------+----------+---------+-------------------+---------+--------+--------+\n",
       "|OrderID|CustomerID|ProductID|          OrderDate|   Amount|Quantity|CityName|\n",
       "+-------+----------+---------+-------------------+---------+--------+--------+\n",
       "|1496884|    468717|    20187|2014-11-05 16:05:03|2844074.0|     1.0|     کرج|\n",
       "|5958461|   2397262|    78180|2017-01-02 22:03:04| 369450.0|     1.0|کرمانشاه|\n",
       "|6345216|   1040870|   273295|2017-02-11 01:20:22| 156789.0|     1.0|   قزوین|\n",
       "+-------+----------+---------+-------------------+---------+--------+--------+"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('''\n",
    "    INSERT INTO OtherDB.dg_Orders\n",
    "    SELECT ID_Order, ID_Customer, ID_Item, DateTime_CartFinalize, Amount_Gross_Order, Quantity_item, city_name_fa\n",
    "    FROM {df}\n",
    "''', df = digikala_df)\n",
    "\n",
    "spark.sql('SELECT * FROM OtherDB.dg_Orders LIMIT 3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ea2c8b93-6248-4666-9d49-78e931d86c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ad4151-c803-40c6-a996-b788d50f0ca6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
